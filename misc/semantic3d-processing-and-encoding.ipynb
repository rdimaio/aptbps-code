{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch as pt\n",
    "import multiprocessing\n",
    "\n",
    "from bps import bps\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_PATH = os.path.join('aptbps-code')\n",
    "data_path = os.path.join(MAIN_PATH, 'data')\n",
    "train_path = os.path.join(data_path, 'train')\n",
    "hdf5_path = os.path.join(data_path, 'hdf5')\n",
    "no_unlabeled_h5_path = os.path.join(data_path, 'no_unlabeled_hdf5')\n",
    "hdf5_train_path = os.path.join(hdf5_path, 'train')\n",
    "encoded_hdf5_path = os.path.join(data_path, 'encoded_hdf5')\n",
    "\n",
    "# All the clouds in the training dataset\n",
    "train_files = [\n",
    "    \"bildstein_station1_xyz_intensity_rgb\",\n",
    "    \"bildstein_station3_xyz_intensity_rgb\",\n",
    "    \"bildstein_station5_xyz_intensity_rgb\",\n",
    "    \"domfountain_station1_xyz_intensity_rgb\",\n",
    "    \"domfountain_station2_xyz_intensity_rgb\",\n",
    "    \"domfountain_station3_xyz_intensity_rgb\",\n",
    "    \"neugasse_station1_xyz_intensity_rgb\",\n",
    "    \"sg27_station1_intensity_rgb\",\n",
    "    \"sg27_station2_intensity_rgb\",\n",
    "    \"sg27_station4_intensity_rgb\",\n",
    "    \"sg27_station5_intensity_rgb\",\n",
    "    \"sg27_station9_intensity_rgb\",\n",
    "    \"sg28_station4_intensity_rgb\",\n",
    "    \"untermaederbrunnen_station1_xyz_intensity_rgb\",\n",
    "    \"untermaederbrunnen_station3_xyz_intensity_rgb\",\n",
    "]\n",
    "\n",
    "# Clouds used for training\n",
    "# train_files = [\n",
    "#     \"bildstein_station1_xyz_intensity_rgb\", \n",
    "#     \"bildstein_station5_xyz_intensity_rgb\",\n",
    "#     \"domfountain_station1_xyz_intensity_rgb\",\n",
    "#     \"domfountain_station3_xyz_intensity_rgb\",\n",
    "#     \"neugasse_station1_xyz_intensity_rgb\",\n",
    "#     \"sg27_station1_intensity_rgb\",\n",
    "#     \"sg27_station2_intensity_rgb\",\n",
    "#     \"sg27_station4_intensity_rgb\",\n",
    "#     \"sg27_station5_intensity_rgb\",\n",
    "#     \"sg27_station9_intensity_rgb\",\n",
    "#     \"sg28_station4_intensity_rgb\",\n",
    "#     \"untermaederbrunnen_station1_xyz_intensity_rgb\",\n",
    "#     \"untermaederbrunnen_station3_xyz_intensity_rgb\",\n",
    "# ]\n",
    "# \n",
    "# # Clouds used for testing\n",
    "# test_files = [\n",
    "#     \"bildstein_station3_xyz_intensity_rgb\",\n",
    "#     \"domfountain_station2_xyz_intensity_rgb\",\n",
    "#     \"sg27_station4_intensity_rgb\",\n",
    "#     \"untermaederbrunnen_station1_xyz_intensity_rgb\",\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   x           y      z     i    r    g    b  label\n",
      "0          20.764000  -17.844000 -1.985 -1615  239  209  221      1\n",
      "1          -9.202000   -0.023000 -0.816 -1132  114   89   68      4\n",
      "2          -9.198000   -0.025000 -0.815 -1183  102   77   56      4\n",
      "3          -9.200000   -0.026000 -0.817 -1275   98   75   54      4\n",
      "4          -9.200000   -0.025000 -0.820 -1173  104   82   63      4\n",
      "...              ...         ...    ...   ...  ...  ...  ...    ...\n",
      "258720943  54.626999  108.795998  0.145 -1555   41   39   43      0\n",
      "258720944  54.665001  108.781998  0.126 -1555   43   39   43      0\n",
      "258720945  54.696999  108.757004  0.184 -1404   39   34   37      0\n",
      "258720946  54.701000  108.763000  0.146 -1442   47   41   45      0\n",
      "258720947  54.698002  108.759003  0.165 -1442   39   34   37      0\n",
      "\n",
      "[258720948 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "# contextual statement that closes store automatically afterwards\n",
    "file_path = os.path.join(data_path, 'hdf5/train/sg28_station4_intensity_rgb.h5')\n",
    "with pd.HDFStore(file_path, compression='lz4', mode='r') as store:\n",
    "    print(store.get('0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all unlabeled points from .h5 files and store them in new .h5 files\n",
    "\n",
    "for f in tqdm(train_files):\n",
    "    f_ext = f + '.h5'\n",
    "    f_path = os.path.join(hdf5_train_path, f_ext)\n",
    "    with pd.HDFStore(f_path, compression='lz4', mode='r') as store:\n",
    "        cloud = store.get('0')\n",
    "        print(cloud.info())\n",
    "        no_unlabeled_cloud = cloud.drop(cloud[cloud.label == 0].index)\n",
    "        print(no_unlabeled_cloud.info())\n",
    "        \n",
    "        no_unlabeled_f_path = os.path.join(no_unlabeled_h5_path, f_ext)\n",
    "        \n",
    "        with pd.HDFStore(no_unlabeled_f_path, compression='lz4', mode='w') as no_unlabeled_store:\n",
    "            no_unlabeled_store['0'] = no_unlabeled_cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  x          y      z     i    r    g    b  label\n",
      "2         20.360001  40.375999 -2.402 -1083  139  151  165      6\n",
      "7         20.358999  40.374001 -2.404 -1086  139  151  165      6\n",
      "8         20.354000  40.375000 -2.404 -1106  139  148  163      6\n",
      "9         20.356001  40.374001 -2.404 -1059  139  151  165      6\n",
      "25        20.361000  40.375000 -2.404 -1116  139  151  165      6\n",
      "...             ...        ...    ...   ...  ...  ...  ...    ...\n",
      "29684536  33.122002  68.304001  7.580 -1555   50   45   51      5\n",
      "29684539  33.020000  68.355003  7.579 -1476   43   37   39      5\n",
      "29684542  33.242001  68.287003  7.597 -1540   61   51   52      5\n",
      "29684545  33.162998  68.301003  7.620 -1573   67   55   67      5\n",
      "29684548  33.320000  68.275002  7.616 -1670   73   57   67      5\n",
      "\n",
      "[9476296 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "# contextual statement that closes store automatically afterwards\n",
    "with pd.HDFStore(no_unlabeled_f_path, compression='lz4', mode='r') as store2:\n",
    "    print(store2.get('0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bps import bps\n",
    "import h5py\n",
    "NUM_POINTS = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.46084379  0.53316554 -1.13999441]\n",
      " [-1.61499181 -1.03422883  1.04827314]\n",
      " [ 0.15953528  0.63335278  0.24866621]\n",
      " ...\n",
      " [-0.0931188   0.11329203 -1.07427838]\n",
      " [ 1.53487635  2.07523421 -0.2291684 ]\n",
      " [ 0.25565463 -1.02660701  0.59164991]]\n",
      "b\n",
      "[[ 0.33417768  0.12928757 -0.27112901]\n",
      " [-0.37652509 -0.23287467  0.23449222]\n",
      " [ 0.03349725  0.15243684  0.049735  ]\n",
      " ...\n",
      " [-0.02488102  0.03227157 -0.25594466]\n",
      " [ 0.35128364  0.48559806 -0.0606735 ]\n",
      " [ 0.05570659 -0.23111357  0.12898483]]\n"
     ]
    }
   ],
   "source": [
    "# batch of 100 point clouds to convert\n",
    "x = np.random.normal(size=[100, 2048, 3])\n",
    "\n",
    "print(x[0])\n",
    "\n",
    "x_norm = bps.normalize(x)\n",
    "print('b')\n",
    "print(x_norm[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Normalize* and *encode*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPS encoding complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  7%|▋         | 1/15 [00:08<01:57,  8.42s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPS encoding complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 13%|█▎        | 2/15 [00:15<01:44,  8.02s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPS encoding complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 3/15 [00:22<01:33,  7.78s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPS encoding complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 27%|██▋       | 4/15 [00:31<01:28,  8.04s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPS encoding complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 33%|███▎      | 5/15 [00:41<01:27,  8.79s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPS encoding complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 40%|████      | 6/15 [00:52<01:22,  9.19s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPS encoding complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 47%|████▋     | 7/15 [01:46<03:02, 22.85s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPS encoding complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 53%|█████▎    | 8/15 [07:36<14:06, 120.99s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPS encoding complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 60%|██████    | 9/15 [15:37<22:53, 228.92s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPS encoding complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 67%|██████▋   | 10/15 [20:49<21:09, 253.97s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPS encoding complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 73%|███████▎  | 11/15 [24:49<16:38, 249.61s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPS encoding complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 80%|████████  | 12/15 [28:55<12:25, 248.64s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPS encoding complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 87%|████████▋ | 13/15 [33:41<08:39, 259.65s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPS encoding complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 93%|█████████▎| 14/15 [34:14<03:11, 191.76s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPS encoding complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 15/15 [34:45<00:00, 139.04s/it]\n"
     ]
    }
   ],
   "source": [
    "from sembps.bps import bps\n",
    "\n",
    "BPS_POINTS = 512\n",
    "\n",
    "for f in tqdm(train_files):\n",
    "    f_ext = f + '.h5'\n",
    "    no_unlabeled_f_path = os.path.join(no_unlabeled_h5_path, f_ext)\n",
    "    encoded_f_path = os.path.join(encoded_hdf5_path, f_ext)\n",
    "    with pd.HDFStore(no_unlabeled_f_path, compression='lz4', mode='r') as store2:\n",
    "        \n",
    "        # Selecting columns (like you would do with the command below) doesn't work.\n",
    "        # This is because the HDFStore was created with the 'fixed' format, which doesn't allow select but has faster read/write performance.\n",
    "        # 'table' format would allow selection. Source: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_hdf.html\n",
    "        #test = pd.DataFrame(store2.select(key=\"0\", columns=[\"x\", \"y\", \"z\"]))\n",
    "        \n",
    "        # Due to this, we must first retrieve the store in its entirety.\n",
    "        df = store2.get('0')\n",
    "    \n",
    "        # Select the xyz columns only for normalization\n",
    "        df2 = df[['x','y','z']]\n",
    "        \n",
    "        # irgb columns for extra features\n",
    "        x_features = df[['i','r','g','b']]\n",
    "        \n",
    "        labels = df[['label']].to_numpy()\n",
    "        \n",
    "        # Store indexes to preserve them\n",
    "        # (As we removed unlabeled points, and we didn't reset the indexes)\n",
    "        indexes = df2.index.to_numpy()\n",
    "        indexes = indexes[:, np.newaxis]\n",
    "        \n",
    "        # Convert to np array\n",
    "        df_np = df2.to_numpy()\n",
    "        \n",
    "        # Add extra dimension to match the requirements of the bps library\n",
    "        df_np = df_np[np.newaxis, :]\n",
    "        \n",
    "        # There's no need to reshape again, as I'm adding the newaxis before the other dims (so it becomes 1, n_points, 3)\n",
    "        # However, just to make sure, I reshape it\n",
    "        df_np = df_np.reshape(1, -1, 3)\n",
    "        \n",
    "        # Normalize cloud\n",
    "        # Normalization must be done on the full cloud.\n",
    "        df_np = bps.normalize(df_np)\n",
    "        \n",
    "        # -- DIVIDING ARRAY INTO TWO ARRAYS OF SIZE ~2048\n",
    "        # To speed up BPS encoding, we can merge this list into two np arrays.\n",
    "        # To obtain even data, we split each cloud into subclouds of ~2048;\n",
    "        # Each 2048 subcloud is then bps-encoded to 512 points.\n",
    "        \n",
    "        n_points = df_np.shape[1]\n",
    "        cloud_points = 2048\n",
    "        \n",
    "        # Remove extra dimension\n",
    "        df_np = np.squeeze(df_np)\n",
    "        \n",
    "        # Floor division\n",
    "        n_divisions = n_points//cloud_points\n",
    "        \n",
    "        # np.array_split returns l % n sub-arrays of size l//n + 1 and the rest of size l//n.\n",
    "        # This means that the split index to achieve this is split_idx must be equal to (l%n * l//n+1)\n",
    "        split_idx = (n_points % n_divisions) * (cloud_points + 1)\n",
    "        \n",
    "        arr1, arr2 = np.split(df_np, [split_idx])\n",
    "        lab1, lab2 = np.split(labels, [split_idx])\n",
    "        idx1, idx2 = np.split(indexes, [split_idx])\n",
    "    \n",
    "        # Add extra dimension to hold the n. of clouds\n",
    "        arr1 = arr1[np.newaxis, :]\n",
    "        arr2 = arr2[np.newaxis, :]\n",
    "        \n",
    "        lab1 = lab1[np.newaxis, :]\n",
    "        lab2 = lab2[np.newaxis, :]\n",
    "        \n",
    "        idx1 = idx1[np.newaxis, :]\n",
    "        idx2 = idx2[np.newaxis, :]\n",
    "        \n",
    "        # Reshape to have n. of clouds as first dimension\n",
    "        arr1 = arr1.reshape(-1, cloud_points+1, 3)\n",
    "        arr2 = arr2.reshape(-1, cloud_points, 3)\n",
    "        lab1 = lab1.reshape(-1, cloud_points+1, 1)\n",
    "        lab2 = lab2.reshape(-1, cloud_points, 1)\n",
    "        idx1 = idx1.reshape(-1, cloud_points+1, 1)\n",
    "        idx2 = idx2.reshape(-1, cloud_points, 1)\n",
    "        \n",
    "            \n",
    "        # bps_idx1 and bps_idx2 contain the indexes of the points sampled by the BPS encoding.\n",
    "        arr1, bps_idx1 = bps.encode(arr1, n_bps_points=BPS_POINTS, radius=1.7, verbose=False, return_idx=True)\n",
    "        arr2, bps_idx2 = bps.encode(arr2, n_bps_points=BPS_POINTS, radius=1.7, verbose=False, return_idx=True)\n",
    "        \n",
    "        print(\"BPS encoding complete!\")\n",
    "        \n",
    "        \n",
    "        # Now we need to filter our starting labels and indexes;\n",
    "        # We want to remove all the points that have not been sampled in the bps.encode process\n",
    "        \n",
    "        filtered_lab1 = np.empty([lab1.shape[0],BPS_POINTS])\n",
    "        filtered_lab2 = np.empty([lab2.shape[0],BPS_POINTS])\n",
    "        filtered_idx1 = np.empty([idx1.shape[0],BPS_POINTS])\n",
    "        filtered_idx2 = np.empty([idx2.shape[0],BPS_POINTS])\n",
    "        \n",
    "        # Filter indexes and labels for first array\n",
    "        for i, (labc, idxc) in enumerate(zip(lab1, idx1)):\n",
    "            bps_idx1[i] = bps_idx1[i].tolist() # Convert current filtered BPS indexes to list\n",
    "            filtered_lab1[i] = np.take(labc, bps_idx1[i]) # Take only the filtered BPS indexes\n",
    "            filtered_idx1[i] = np.take(idxc, bps_idx1[i]) # Take only the filtered BPS indexes\n",
    "    \n",
    "        # Filter indexes and labels for second array\n",
    "        for j, (labc, idxc) in enumerate(zip(lab2, idx2)):\n",
    "            bps_idx2[i] = bps_idx2[i].tolist() # Convert current filtered BPS indexes to list\n",
    "            filtered_lab2[j] = np.take(labc, bps_idx2[j]) # Take only the filtered BPS indexes\n",
    "            filtered_idx2[j] = np.take(idxc, bps_idx2[j]) # Take only the filtered BPS indexes\n",
    "        \n",
    "        # Concatenate the two arrays, as they now have the same n. of points per cloud (BPS_POINTS)\n",
    "        df_np = np.concatenate((arr1, arr2))\n",
    "        indexes = np.concatenate((filtered_idx1, filtered_idx2))\n",
    "        labels = np.concatenate((filtered_lab1, filtered_lab2))\n",
    "    \n",
    "        ## Convert to pandas dataframe\n",
    "        #df2 = pd.DataFrame(data=df_np, index=df_np_indexes, columns=['dist'])\n",
    "        \n",
    "        # Remove xyz columns from original dataframe\n",
    "        #df = df.drop(['x','y','z'], axis=1)\n",
    "        #\n",
    "        ## Join normalized xyz dataframe and original dataframe with xyz columns removed\n",
    "        #df2 = df2.join(df)\n",
    "        #print(df2)\n",
    "        \n",
    "        # Write array to new file\n",
    "        with h5py.File(encoded_f_path, 'w') as f_to_w:\n",
    "            for i in range(df_np.shape[0]):\n",
    "                grp = f_to_w.create_group(str(i)) # e.g. group '0'\n",
    "                data_dset = grp.create_dataset(\"data\", data=df_np[i], dtype='float32', chunks=True)\n",
    "                label_dset = grp.create_dataset(\"label\", data=labels[i], dtype='uint8', chunks=True)\n",
    "                idx_dset = grp.create_dataset(\"indexes\", data=indexes[i], dtype='uint32', chunks=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in tqdm(train_files):\n",
    "    f_ext = f + '.h5'\n",
    "    no_unlabeled_f_path = os.path.join(no_unlabeled_h5_path, f_ext)\n",
    "    encoded_f_path = os.path.join(encoded_hdf5_path, f_ext)\n",
    "    \n",
    "    with h5py.File(encoded_f_path, 'r') as f_to_r:\n",
    "        #print(f_to_r['0']['label'][:])\n",
    "        #print(len(f_to_r.keys()))\n",
    "        print(f_to_r['0']['data'][:])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate .h5 files into a single sliceable virtual dataset\n",
    "def concatenate(file_names_to_concatenate):\n",
    "    entry_key = 'data'  # where the data is inside of the source files.\n",
    "    sh = h5py.File(file_names_to_concatenate[0], 'r')[entry_key].shape  # get the first ones shape.\n",
    "    layout = h5py.VirtualLayout(shape=(len(file_names_to_concatenate),) + sh,\n",
    "                                dtype=np.float)\n",
    "    with h5py.File(\"VDS.h5\", 'w', libver='latest') as f:\n",
    "        for i, filename in enumerate(file_names_to_concatenate):\n",
    "            vsource = h5py.VirtualSource(filename, entry_key, shape=sh)\n",
    "            layout[i, :, :, :] = vsource\n",
    "\n",
    "        f.create_virtual_dataset(entry_key, layout, fillvalue=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for each file:\n",
    "    for entries in file:\n",
    "        concatenate all the entries\n",
    "    concatenate all files\n",
    "    \n",
    "for f in tqdm(train_files):\n",
    "    f_ext = f + '.h5'\n",
    "    no_unlabeled_f_path = os.path.join(no_unlabeled_h5_path, f_ext)\n",
    "    encoded_f_path = os.path.join(encoded_hdf5_path, f_ext)\n",
    "    \n",
    "    with h5py.File(encoded_f_path, 'r') as f_to_r:\n",
    "        print(f_to_r['0']['label'][:])\n",
    "        print(len(f_to_r.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [02:18<00:00,  9.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Virtual datasets:\n",
      "(952084, 512)\n",
      "(952084, 512)\n"
     ]
    }
   ],
   "source": [
    "# CREATE VIRTUAL DATASET\n",
    "\n",
    "BPS_POINTS = 512\n",
    "\n",
    "VDS_path = os.path.join(encoded_hdf5_path, 'VDS.h5')\n",
    "\n",
    "# Initialize list for VirtualSources\n",
    "data_vsources = []\n",
    "label_vsources = []\n",
    "\n",
    "for f in tqdm(train_files):\n",
    "    f_ext = f + '.h5'\n",
    "    no_unlabeled_f_path = os.path.join(no_unlabeled_h5_path, f_ext)\n",
    "    encoded_f_path = os.path.join(encoded_hdf5_path, f_ext)\n",
    "\n",
    "    with h5py.File(encoded_f_path, 'r') as f_to_r:\n",
    "            # Iterate over all groups within the file\n",
    "            for group in f_to_r.keys():\n",
    "                # Specify paths to data and label datasets within group\n",
    "                data_s = group + '/data'\n",
    "                label_s = group + '/label'\n",
    "                # Create VirtualSources for both datasets and append them to respective lists\n",
    "                data_vsource = h5py.VirtualSource(encoded_f_path, data_s, shape=(1, 512))\n",
    "                label_vsource = h5py.VirtualSource(encoded_f_path, label_s, shape=(1, 512))\n",
    "                data_vsources.append(data_vsource)\n",
    "                label_vsources.append(label_vsource)\n",
    "            \n",
    "            #print((f_to_r[str(len(data_vsources)-1)]['label'][:]))\n",
    "                \n",
    "    \n",
    "data_vlayout = h5py.VirtualLayout(shape=(len(data_vsources), 512))\n",
    "label_vlayout = h5py.VirtualLayout(shape=(len(label_vsources), 512))\n",
    "    \n",
    "# Populate layouts\n",
    "for i, (data_vsource, label_vsource) in enumerate(zip(data_vsources, label_vsources)):\n",
    "    data_vlayout[i] = data_vsource\n",
    "    label_vlayout[i] = label_vsource\n",
    "        \n",
    "# Add virtual dataset to output file\n",
    "with h5py.File(VDS_path, \"w\", libver=\"latest\") as f:\n",
    "    f.create_virtual_dataset(\"vdata\", data_vlayout, fillvalue=-5)\n",
    "    f.create_virtual_dataset(\"vlabels\", label_vlayout, fillvalue=-5)\n",
    "    \n",
    "# read data back\n",
    "# virtual dataset is transparent for reader!\n",
    "with h5py.File(VDS_path, \"r\") as f:\n",
    "    print(\"Virtual datasets:\")\n",
    "    print(f[\"vdata\"].shape)\n",
    "    #print(f[\"vdata\"][:, :10])\n",
    "    print(f[\"vlabels\"].shape)\n",
    "    #print(f[\"vlabels\"][:, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VDS_path = os.path.join(encoded_hdf5_path, 'VDS.h5')\n",
    "print('we')\n",
    "# read data back\n",
    "# virtual dataset is transparent for reader!\n",
    "with h5py.File(VDS_path, \"r\") as f:\n",
    "    print(\"Virtual datasets:\")\n",
    "    dset = f[\"vdata\"][:]\n",
    "    #print(f[\"vdata\"].shape)\n",
    "    #print(f[\"vdata\"][:, :10])\n",
    "    #print(f[\"vlabels\"].shape)\n",
    "    #print(f[\"vlabels\"][:, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[      1 1294930]\n",
      " [      2 3508530]\n",
      " [      3 2538281]\n",
      " [      4   49687]\n",
      " [      5 1242205]\n",
      " [      6  763114]\n",
      " [      7   13899]\n",
      " [      8   65650]]\n"
     ]
    }
   ],
   "source": [
    "no_unlabeled_f_path = os.path.join(no_unlabeled_h5_path, f_ext)\n",
    "encoded_f_path = os.path.join(encoded_hdf5_path, f_ext)\n",
    "\n",
    "with pd.HDFStore(no_unlabeled_f_path, compression='lz4', mode='r') as store2:\n",
    "        \n",
    "        # Selecting columns (like you would do with the command below) doesn't work.\n",
    "        # This is because the HDFStore was created with the 'fixed' format, which doesn't allow select but has faster read/write performance.\n",
    "        # 'table' format would allow selection. Source: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_hdf.html\n",
    "        #test = pd.DataFrame(store2.select(key=\"0\", columns=[\"x\", \"y\", \"z\"]))\n",
    "        \n",
    "        # Due to this, we must first retrieve the store in its entirety.\n",
    "        df = store2.get('0')\n",
    "        \n",
    "        labels = df[['label']].to_numpy()\n",
    "        unique, counts = np.unique(labels, return_counts=True)\n",
    "\n",
    "        print(np.asarray((unique, counts)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%who"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prototype dataset class\n",
    "class HDF5Set(Dataset):\n",
    "    def __init__(self, h5filelist):\n",
    "        self.h5filelist = h5filelist\n",
    "    \n",
    "    def __get__item(self, index):\n",
    "        # you have n of basis points and number of keys for each file;\n",
    "        # based on that, from the index you can get which file you need to open\n",
    "        # to obtain the point\n",
    "        \n",
    "    def __len__(self):\n",
    "        x = BASIS_POINTS # multiplied by the total number of entries \n",
    "        return(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2048, 2048, 3)\n",
      "(2048, 1)\n"
     ]
    }
   ],
   "source": [
    "f = h5py.File('ply_data_test0.h5', 'r')\n",
    "data = f[\"data\"][:]\n",
    "label = f[\"label\"][:]\n",
    "print(data.shape)\n",
    "print(label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://discuss.pytorch.org/t/use-of-dataset-class/1620/3\n",
    "#class Sem3d(Dataset):\n",
    "#  def __init__(self, hdf5_list):\n",
    "#        self.datasets = []\n",
    "#        self.total_count = 0\n",
    "#        for f in hdf5_list:\n",
    "#            h5_file = h5py.File(f, 'r')\n",
    "#            dataset = h5_file['YOUR DATASET NAME']\n",
    "#            self.datasets.append(dataset)\n",
    "#            self.total_count += len(dataset)\n",
    "#\n",
    "#  def __getitem__(self, index):\n",
    "#     \n",
    "#      dataset_index=-1\n",
    "#      for i in xrange(len(self.limits)-1,-1,-1):\n",
    "#        #print 'i ',i\n",
    "#        if index>=self.limits[i]:\n",
    "#          dataset_index=i\n",
    "#          break\n",
    "#      #print 'dataset_index ',dataset_index\n",
    "#      assert dataset_index>=0, 'negative chunk'\n",
    "#\n",
    "#      in_dataset_index = index-self.limits[dataset_index]\n",
    "#\n",
    "#      return self.datasets[dataset_index][in_dataset_index], self.datasets_gt[dataset_index][in_dataset_index]\n",
    "#\n",
    "#  def __len__(self):\n",
    "#      return self.total_count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import helpers\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.utils import data\n",
    "\n",
    "class HDF5Dataset(data.Dataset):\n",
    "    \"\"\"Represents an abstract HDF5 dataset.\n",
    "    \n",
    "    Input params:\n",
    "        file_path: Path to the folder containing the dataset (one or multiple HDF5 files).\n",
    "        recursive: If True, searches for h5 files in subdirectories.\n",
    "        load_data: If True, loads all the data immediately into RAM. Use this if\n",
    "            the dataset is fits into memory. Otherwise, leave this at false and \n",
    "            the data will load lazily.\n",
    "        data_cache_size: Number of HDF5 files that can be cached in the cache (default=3).\n",
    "        transform: PyTorch transform to apply to every data instance (default=None).\n",
    "    \"\"\"\n",
    "    def __init__(self, file_path, recursive=False, load_data=False, data_cache_size=3, transform=None):\n",
    "        super().__init__()\n",
    "        self.data_info = []\n",
    "        self.data_cache = {}\n",
    "        self.data_cache_size = data_cache_size\n",
    "        self.transform = transform\n",
    "\n",
    "        # Search for all h5 files\n",
    "        p = Path(file_path)\n",
    "        assert(p.is_dir())\n",
    "        if recursive:\n",
    "            files = sorted(p.glob('**/*.h5'))\n",
    "        else:\n",
    "            files = sorted(p.glob('*.h5'))\n",
    "        if len(files) < 1:\n",
    "            raise RuntimeError('No hdf5 datasets found')\n",
    "\n",
    "        for h5dataset_fp in files:\n",
    "            self._add_data_infos(str(h5dataset_fp.resolve()), load_data)\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        # get data\n",
    "        x = self.get_data(\"data\", index)\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        else:\n",
    "            x = torch.from_numpy(x)\n",
    "\n",
    "        # get label\n",
    "        y = self.get_data(\"label\", index)\n",
    "        y = torch.from_numpy(y)\n",
    "        return (x, y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.get_data_infos('data'))\n",
    "    \n",
    "    def _add_data_infos(self, file_path, load_data):\n",
    "        with h5py.File(file_path) as h5_file:\n",
    "            # Walk through all groups, extracting datasets\n",
    "            for gname, group in h5_file.items():\n",
    "                for dname, ds in group.items():\n",
    "                    # if data is not loaded its cache index is -1\n",
    "                    idx = -1\n",
    "                    if load_data:\n",
    "                        # add data to the data cache\n",
    "                        idx = self._add_to_cache(ds.value, file_path)\n",
    "                    \n",
    "                    # type is derived from the name of the dataset; we expect the dataset\n",
    "                    # name to have a name such as 'data' or 'label' to identify its type\n",
    "                    # we also store the shape of the data in case we need it\n",
    "                    self.data_info.append({'file_path': file_path, 'type': dname, 'shape': ds.value.shape, 'cache_idx': idx})\n",
    "\n",
    "    def _load_data(self, file_path):\n",
    "        \"\"\"Load data to the cache given the file\n",
    "        path and update the cache index in the\n",
    "        data_info structure.\n",
    "        \"\"\"\n",
    "        with h5py.File(file_path) as h5_file:\n",
    "            for gname, group in h5_file.items():\n",
    "                for dname, ds in group.items():\n",
    "                    # add data to the data cache and retrieve\n",
    "                    # the cache index\n",
    "                    idx = self._add_to_cache(ds.value, file_path)\n",
    "\n",
    "                    # find the beginning index of the hdf5 file we are looking for\n",
    "                    file_idx = next(i for i,v in enumerate(self.data_info) if v['file_path'] == file_path)\n",
    "\n",
    "                    # the data info should have the same index since we loaded it in the same way\n",
    "                    self.data_info[file_idx + idx]['cache_idx'] = idx\n",
    "\n",
    "        # remove an element from data cache if size was exceeded\n",
    "        if len(self.data_cache) > self.data_cache_size:\n",
    "            # remove one item from the cache at random\n",
    "            removal_keys = list(self.data_cache)\n",
    "            removal_keys.remove(file_path)\n",
    "            self.data_cache.pop(removal_keys[0])\n",
    "            # remove invalid cache_idx\n",
    "            self.data_info = [{'file_path': di['file_path'], 'type': di['type'], 'shape': di['shape'], 'cache_idx': -1} if di['file_path'] == removal_keys[0] else di for di in self.data_info]\n",
    "\n",
    "    def _add_to_cache(self, data, file_path):\n",
    "        \"\"\"Adds data to the cache and returns its index. There is one cache\n",
    "        list for every file_path, containing all datasets in that file.\n",
    "        \"\"\"\n",
    "        if file_path not in self.data_cache:\n",
    "            self.data_cache[file_path] = [data]\n",
    "        else:\n",
    "            self.data_cache[file_path].append(data)\n",
    "        return len(self.data_cache[file_path]) - 1\n",
    "\n",
    "    def get_data_infos(self, type):\n",
    "        \"\"\"Get data infos belonging to a certain type of data.\n",
    "        \"\"\"\n",
    "        data_info_type = [di for di in self.data_info if di['type'] == type]\n",
    "        return data_info_type\n",
    "\n",
    "    def get_data(self, type, i):\n",
    "        \"\"\"Call this function anytime you want to access a chunk of data from the\n",
    "            dataset. This will make sure that the data is loaded in case it is\n",
    "            not part of the data cache.\n",
    "        \"\"\"\n",
    "        fp = self.get_data_infos(type)[i]['file_path']\n",
    "        if fp not in self.data_cache:\n",
    "            self._load_data(fp)\n",
    "        \n",
    "        # get new cache_idx assigned by _load_data_info\n",
    "        cache_idx = self.get_data_infos(type)[i]['cache_idx']\n",
    "        return self.data_cache[fp][cache_idx]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
