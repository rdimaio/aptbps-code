{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device found...\n",
      "loading ModelNet40 point clouds...\n",
      "downloading ModelNet40 data..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0% 435216384 / 435212151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unzipping files..\n",
      "File Name                                             Modified             Size\n",
      "modelnet40_ply_hdf5_2048/                      2017-02-02 13:28:26            0\n",
      "modelnet40_ply_hdf5_2048/ply_data_train_2_id2file.json 2017-02-01 14:36:12        52667\n",
      "modelnet40_ply_hdf5_2048/ply_data_train2.h5    2017-02-01 14:29:06     72245751\n",
      "modelnet40_ply_hdf5_2048/ply_data_train4.h5    2017-02-01 14:29:06     58634835\n",
      "modelnet40_ply_hdf5_2048/ply_data_train1.h5    2017-02-01 14:29:06     73196621\n",
      "modelnet40_ply_hdf5_2048/train_files.txt       2017-02-01 14:37:42          245\n",
      "modelnet40_ply_hdf5_2048/ply_data_train_4_id2file.json 2017-02-01 14:36:24        42383\n",
      "modelnet40_ply_hdf5_2048/ply_data_test1.h5     2017-02-01 14:29:06     14356248\n",
      "modelnet40_ply_hdf5_2048/ply_data_test0.h5     2017-02-01 14:29:06     70992141\n",
      "modelnet40_ply_hdf5_2048/ply_data_test_1_id2file.json 2017-02-01 14:35:50        11135\n",
      "modelnet40_ply_hdf5_2048/ply_data_train_1_id2file.json 2017-02-01 14:36:04        52515\n",
      "modelnet40_ply_hdf5_2048/ply_data_train_0_id2file.json 2017-02-01 14:35:58        52945\n",
      "modelnet40_ply_hdf5_2048/test_files.txt        2017-02-01 14:37:48           96\n",
      "modelnet40_ply_hdf5_2048/ply_data_train0.h5    2017-02-01 14:29:06     73523284\n",
      "modelnet40_ply_hdf5_2048/ply_data_test_0_id2file.json 2017-02-01 14:30:48        53583\n",
      "modelnet40_ply_hdf5_2048/shape_names.txt       2017-02-02 13:28:14          275\n",
      "modelnet40_ply_hdf5_2048/ply_data_train3.h5    2017-02-01 14:29:08     72932856\n",
      "modelnet40_ply_hdf5_2048/ply_data_train_3_id2file.json 2017-02-01 14:36:18        52383\n",
      "loaded 9840 training and 2468 test samples.\n",
      "converting data to BPS representation..\n",
      "number of basis points: 512\n",
      "BPS sampling radius: 1.000000\n",
      "converting train..\n",
      "using 12 available CPUs for BPS encoding..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 58/83 [00:00<00:00, 291.57it/s]\n",
      "100%|██████████| 84/84 [00:00<00:00, 324.15it/s]\n",
      "100%|██████████| 83/83 [00:00<00:00, 324.86it/s]\n",
      "100%|██████████| 84/84 [00:00<00:00, 308.67it/s]\n",
      "100%|██████████| 83/83 [00:00<00:00, 334.16it/s]\n",
      "\n",
      "100%|██████████| 83/83 [00:00<00:00, 323.66it/s]\n",
      "100%|██████████| 83/83 [00:00<00:00, 309.14it/s]\n",
      "100%|██████████| 83/83 [00:00<00:00, 336.38it/s]\n",
      "100%|██████████| 83/83 [00:00<00:00, 344.61it/s]\n",
      "100%|██████████| 83/83 [00:00<00:00, 306.47it/s]\n",
      "100%|██████████| 83/83 [00:00<00:00, 347.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using 12 available CPUs for BPS encoding..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83/83 [00:25<00:00,  3.25it/s]\n",
      "100%|██████████| 83/83 [00:25<00:00,  3.25it/s]\n",
      "100%|██████████| 84/84 [00:25<00:00,  3.27it/s]\n",
      "100%|██████████| 84/84 [00:25<00:00,  3.26it/s]\n",
      "100%|██████████| 83/83 [00:25<00:00,  3.22it/s]\n",
      "100%|██████████| 84/84 [00:25<00:00,  3.25it/s]\n",
      "100%|██████████| 83/83 [00:25<00:00,  3.21it/s]\n",
      "100%|██████████| 83/83 [00:25<00:00,  3.21it/s]\n",
      "100%|██████████| 83/83 [00:25<00:00,  3.21it/s]\n",
      "100%|██████████| 84/84 [00:25<00:00,  3.24it/s]\n",
      "100%|██████████| 83/83 [00:25<00:00,  3.20it/s]\n",
      "100%|██████████| 83/83 [00:25<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting test..\n",
      "using 12 available CPUs for BPS encoding..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84/84 [00:00<00:00, 334.78it/s]\n",
      "100%|██████████| 84/84 [00:00<00:00, 334.67it/s]\n",
      "\n",
      "100%|██████████| 84/84 [00:00<00:00, 323.56it/s]\n",
      "100%|██████████| 83/83 [00:00<00:00, 321.52it/s]\n",
      "\n",
      "100%|██████████| 83/83 [00:00<00:00, 336.72it/s]\n",
      "100%|██████████| 83/83 [00:00<00:00, 319.74it/s]\n",
      "100%|██████████| 83/83 [00:00<00:00, 320.29it/s]\n",
      "100%|██████████| 83/83 [00:00<00:00, 322.94it/s]\n",
      "100%|██████████| 83/83 [00:00<00:00, 315.88it/s]\n",
      "100%|██████████| 83/83 [00:00<00:00, 336.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using 12 available CPUs for BPS encoding..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83/83 [00:25<00:00,  3.25it/s]\n",
      " 99%|█████████▉| 82/83 [00:25<00:00,  3.29it/s]\n",
      "100%|██████████| 83/83 [00:25<00:00,  3.23it/s]\n",
      "100%|██████████| 83/83 [00:25<00:00,  3.23it/s]\n",
      "\n",
      "100%|██████████| 83/83 [00:25<00:00,  3.23it/s]\n",
      "100%|██████████| 83/83 [00:25<00:00,  3.23it/s]\n",
      "100%|██████████| 83/83 [00:25<00:00,  3.22it/s]\n",
      "100%|██████████| 84/84 [00:25<00:00,  3.24it/s]\n",
      "100%|██████████| 84/84 [00:25<00:00,  3.23it/s]\n",
      "100%|██████████| 84/84 [00:25<00:00,  3.23it/s]\n",
      "100%|██████████| 84/84 [00:26<00:00,  3.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conversion finished. \n",
      "saving cache file for future runs..\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import multiprocessing\n",
    "import time\n",
    "\n",
    "# PyTorch dependencies\n",
    "import torch as pt\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# local dependencies\n",
    "#from bps import bps\n",
    "from sembps.bps import aptbps\n",
    "from modelnet40 import load_modelnet40\n",
    "\n",
    "N_SAMPLES = 1000\n",
    "\n",
    "MAIN_PATH = os.path.join(os.sep, 'aptbps-code')\n",
    "MOD40_PATH = os.path.join(MAIN_PATH, 'modelnet40')\n",
    "\n",
    "LOGS_PATH = os.path.join(MOD40_PATH, 'logs')\n",
    "DATA_PATH = os.path.join(MOD40_PATH, 'data')\n",
    "\n",
    "BPS_CACHE_FILE = os.path.join(DATA_PATH, 'bps_mlp_data.npz')\n",
    "APTBPS_CACHE_FILE = os.path.join(DATA_PATH, 'aptbps_mlp_data.npz')\n",
    "\n",
    "N_MODELNET_CLASSES = 40\n",
    "\n",
    "N_BPS_POINTS = 512\n",
    "BPS_RADIUS = 1\n",
    "\n",
    "N_CPUS = multiprocessing.cpu_count()\n",
    "N_GPUS = torch.cuda.device_count()\n",
    "\n",
    "if N_GPUS > 0:\n",
    "    DEVICE = 'cuda'\n",
    "    print(\"GPU device found...\")\n",
    "else:\n",
    "    DEVICE = 'cpu'\n",
    "    print(\"GPU device not found, using %d CPU(s)...\" % N_CPUS)\n",
    "\n",
    "if not os.path.exists(LOGS_PATH):\n",
    "    os.makedirs(LOGS_PATH)\n",
    "\n",
    "\n",
    "class ShapeClassifierMLP(nn.Module):\n",
    "\n",
    "    def __init__(self, n_features, n_classes, hsize1=512,  hsize2=512, dropout1=0.8, dropout2=0.8):\n",
    "        super(ShapeClassifierMLP, self).__init__()\n",
    "\n",
    "        self.bn0 = nn.BatchNorm1d(n_features)\n",
    "        self.fc1 = nn.Linear(in_features=n_features, out_features=hsize1)\n",
    "        self.bn1 = nn.BatchNorm1d(hsize1)\n",
    "        self.do1 = nn.Dropout(dropout1)\n",
    "        self.fc2 = nn.Linear(in_features=hsize1, out_features=hsize2)\n",
    "        self.bn2 = nn.BatchNorm1d(hsize2)\n",
    "        self.do2 = nn.Dropout(dropout2)\n",
    "        self.fc3 = nn.Linear(in_features=hsize2, out_features=n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.bn0(x)\n",
    "        x = self.do1(self.bn1(F.relu(self.fc1(x))))\n",
    "        x = self.do2(self.bn2(F.relu(self.fc2(x))))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def fit(model, device, train_loader, optimizer):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "def test(model, device, test_loader, epoch_id):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    n_test_samples = len(test_loader.dataset)\n",
    "    n_correct = 0\n",
    "    with pt.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction=\"sum\").item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            n_correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= n_test_samples\n",
    "    test_acc = 100.0 * n_correct / n_test_samples\n",
    "    print(\n",
    "        \"Epoch {} test loss: {:.4f}, test accuracy: {}/{} ({:.2f}%)\".format(epoch_id, test_loss, n_correct, n_test_samples, test_acc))\n",
    "\n",
    "    return test_loss, test_acc\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def prepare_data_loaders():\n",
    "\n",
    "    if not os.path.exists(BPS_CACHE_FILE):\n",
    "\n",
    "        # load modelnet point clouds\n",
    "        xtr, ytr, xte, yte = load_modelnet40(root_data_dir=DATA_PATH)\n",
    "\n",
    "        # this will normalise your point clouds and return scaler parameters for inverse operation\n",
    "        xtr_normalized = aptbps.normalize(xtr)\n",
    "        xte_normalized = aptbps.normalize(xte)\n",
    "\n",
    "        # this will encode your normalised point clouds with random basis of 512 points,\n",
    "        # each BPS cell containing l2-distance to closest point\n",
    "        start = time.time()\n",
    "        print(\"converting data to BPS representation..\")\n",
    "        print(\"number of basis points: %d\" % N_BPS_POINTS)\n",
    "        print(\"BPS sampling radius: %f\" % BPS_RADIUS)\n",
    "        print(\"converting train..\")\n",
    "        xtr_bps = aptbps.encode(xtr_normalized[0:N_SAMPLES], bps_arrangement=\"random\", n_bps_points=N_BPS_POINTS, bps_cell_type='dists', radius=BPS_RADIUS)\n",
    "        \n",
    "        xtr_aptbps = aptbps.adaptive_encode(xtr_normalized[0:N_SAMPLES], bps_arrangement=\"random\", n_parts=2, n_bps_points=N_BPS_POINTS, bps_cell_type='dists', radius=BPS_RADIUS)\n",
    "        print(\"converting test..\")\n",
    "        xte_bps = aptbps.encode(xte_normalized[0:N_SAMPLES], bps_arrangement=\"random\", n_bps_points=N_BPS_POINTS, bps_cell_type='dists', radius=BPS_RADIUS) # test\n",
    "        \n",
    "        xte_aptbps = aptbps.adaptive_encode(xtr_normalized[0:N_SAMPLES], bps_arrangement=\"random\", n_parts=2, n_bps_points=N_BPS_POINTS, bps_cell_type='dists', radius=BPS_RADIUS)\n",
    "        end = time.time()\n",
    "        total_training_time = (end - start) / 60\n",
    "        print(\"conversion finished. \")\n",
    "        print(\"saving cache file for future runs..\")\n",
    "\n",
    "        np.savez(APTBPS_CACHE_FILE, xtr=xtr_aptbps,\n",
    "                 ytr=ytr[0:N_SAMPLES], xte=xte_aptbps, yte=yte[0:N_SAMPLES])\n",
    "\n",
    "        np.savez(BPS_CACHE_FILE, xtr=xtr_bps, ytr=ytr[0:N_SAMPLES],\n",
    "                 xte=xte_bps, yte=yte[0:N_SAMPLES])\n",
    "\n",
    "    else:\n",
    "        print(\"loading converted data from cache..\")\n",
    "        data = np.load(BPS_CACHE_FILE)\n",
    "        xtr_bps = data['xtr']\n",
    "        ytr = data['ytr']\n",
    "        xte_bps = data['xte']\n",
    "        yte = data['yte']\n",
    "\n",
    "\n",
    "# For kde\n",
    "from scipy import stats\n",
    "\n",
    "def main():\n",
    "    prepare_data_loaders()\n",
    "\n",
    "\n",
    "from functools import partial\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from tqdm import tqdm\n",
    "\n",
    "# For kde\n",
    "from scipy import stats\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
